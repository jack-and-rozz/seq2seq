# Word embeddings.
glove_300d_filtered {
  path = glove.840B.300d.txt.filtered
  size = 300
  format = txt
}
turian_50d {
  path = turian.50d.txt
  size = 50
  format = txt
}

# Task dependent configs.
coref {
  # Shared params.
  dropout_rate = ${main.dropout_rate}
  lexical_dropout_rate = ${main.lexical_dropout_rate}

  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  mention_ratio = 0.4

  # Model hyperparameters.
  loss_weight = 1.0 # Fixed. This is for weighted loss of each task in MTL.
  batch_size = 1 # Fixed. Current model can handle only the case batch_size = 1.
  filter_widths = [3, 4, 5]
  filter_size = 50
  ffnn_size = 150
  ffnn_depth = 2
  f_embedding_size = 20
  max_mention_width = 10
  use_metadata = true
  use_features = true
  model_heads = true

  # Dataset
  data_dir = dataset/coref/source
  conll_dir = dataset/coref/conll-2012
  train_data = train.english.jsonlines
  train_gold = train.english.v4_auto_conll
  valid_data = dev.english.jsonlines
  valid_gold = dev.english.v4_auto_conll
  test_data = test.english.jsonlines
  test_gold = test.english.v4_gold_conll
}

wikiP2D {
  batch_size = 1
  loss_weight = 10.0
  hidden_size = ${main.hidden_size}
  dropout_rate = ${main.dropout_rate}
  lexical_dropout_rate = ${main.lexical_dropout_rate}
  max_sent_length = {
    encode = 40
    decode = 20
  }
  n_triples = 0
  dataset = Q5O15000R300.all.bin
}

encoder{
}

# Global hyperparameters.
main {
  # Multi-task.
  model_type = WeightedLoss
  graph_task = true
  desc_task = false
  coref_task = true

  # Word and character processing.
  cbase = true
  wbase = true
  w_vocab_size = 50000
  c_vocab_size = 1000
  w_embedding_size = 300
  c_embedding_size = 8
  char_vocab_path = "char_vocab.english.txt"
  lowercase = false
  use_pretrained_emb = true
  trainable_emb = false

  embeddings = [${glove_300d_filtered}, ${turian_50d}]
  embeddings_dir = "dataset/embeddings"
  encoder = ${encoder}
  coref = ${coref}
  wikiP2D = ${wikiP2D}
  # Learning hyperparameters.
  max_to_keep = 10
  max_epoch = 50
  hidden_size = 200
  max_gradient_norm = 5.0
  num_layers = 1
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  dropout_rate = 0.2 #keep_prob = 1.0 - rate
  lexical_dropout_rate = 0.5 #keep_prob = 1.0 - rate
  #cell_type = GRUCell
  cell_type = CustomLSTMCell
  num_layers = 1
}
