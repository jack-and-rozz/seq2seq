###################################
#   Embedding initialization
###################################
embeddings_dir = "embeddings"
glove_300d_filtered {
  path = ${embeddings_dir}/glove.840B.300d.txt.filtered
  size = 300
  skip_first = false
}
turian_50d {
  path = ${embeddings_dir}/turian.50d.txt
  size = 50
  skip_first = false
}

###################################
#         Optimizer
###################################

adam {
  optimizer_type = AdamOptimizer
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}
sgd {
  optimizer_type = GradientDescentOptimizer
  learning_rate = 0.01
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}

#################################
#    Task Dependent Models
#################################

coref {
  dataset = ${conll2012shared}
  model_type=CoreferenceResolution
  use_local_rnn = ${main.use_local_rnn}

  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  mention_ratio = 0.4

  # Model hyperparameters.
  batch_size = 1 # Fixed. Current model can handle only the case batch_size = 1.
  dropout_rate = ${main.dropout_rate}
  filter_widths = [3, 4, 5]
  filter_size = 50
  ffnn_size = 150
  ffnn_depth = 2
  f_embedding_size = 20
  max_mention_width = 10
  use_metadata = true
  use_features = true
  model_heads = true
}

graph{
  dataset=${wikiP2Dgraph}
  model_type=GraphLinkPrediction
  use_local_rnn = ${main.use_local_rnn}
  dropout_rate = ${main.dropout_rate}
  batch_size = 100
  ffnn_size = 150
  cnn = {
    filter_widths = [2, 3]
    filter_size = 50
  }
}

desc{
  dataset=${graph}
  model_type=DescriptionGeneration
  entity_word_dropout_rate=1.0
  context_word_dropout_rate=${main.word_dropout_rate}
  decoder_cell = ${main.decoder_cell}
  rnn_size = ${main.rnn_size}
  num_layers = ${main.num_layers}
  dropout_rate = ${main.dropout_rate}
  beam_width=5
  length_penalty_weight=0.6 # https://arxiv.org/pdf/1609.08144.pdf
}

#################################
#          Dataset
#################################

wikiP2Dgraph{
  name = graph
  dataset_type=WikiP2DDataset
  source_dir=dataset/wikiP2D/source
  processed_dir=dataset/wikiP2D/processed
  minlen = {
     word=0
     char=5
  }
  maxlen = {
     word=0
     char=10
  }
  n_triples = 0
  max_rows = 100000
  train_data = train.jsonlines
  valid_data = dev.jsonlines
  test_data = test.jsonlines
  prop_data = properties.tokenized.jsonlines
  mask_link = false
}

conll2012shared{
  name=coref
  dataset_type=CoNLL2012CorefDataset
  source_dir = dataset/coref/source
  conll_dir = dataset/coref/conll-2012
  train_data = train.english.jsonlines
  train_gold = train.english.v4_auto_conll
  valid_data = dev.english.jsonlines
  valid_gold = dev.english.v4_auto_conll
  test_data = test.english.jsonlines
  test_gold = test.english.v4_gold_conll

}

########################################################

#################################
#          Main 
#################################
main {
  # Multi-task configuration.
  model_type = BatchIterative
  tasks = {
    #coref = ${coref}
    #graph = ${graph}
  }

  # Shared hyperparameters.
  encoder_cell = CustomLSTMCell # [GRUCell or CustomLSTMCell]
  decoder_cell = GRUCell
  rnn_size = 200
  num_layers = 1
  use_local_rnn = false

  # Learning hyperparameters.
  max_to_keep = 1
  max_epoch = 40
  dropout_rate = 0.2   # keep_prob = 1.0 - dropout_rate
  lexical_dropout_rate = 0.5   # for word embeddings and char embeddings encoded by CNN.
  word_dropout_rate = 0.0
  optimizer = ${adam}

  # Word and character processing.
  vocab_size = { # if the corresponding size is 0, the token type is not used.
    word = 20000 
    char = 200
  }
  w_embedding_size = 300
  c_embedding_size = 8
  lowercase = false
  normalize_digits = true
  trainable_emb = false
  embeddings_conf = [${glove_300d_filtered}, ${turian_50d}]
  #embeddings_conf = [${glove_300d_filtered}]
  char_vocab_path = ${embeddings_dir}/char_vocab.english.txt
}


##################################################3
#                 Variants.
##################################################3

only_coref = ${main} {
  tasks = {
    coref = ${coref}
  }
}

only_graph_10k = ${main} {
  tasks = {
    graph = ${graph} {
       dataset = ${graph.dataset} {
          mask_link = false
	  max_rows = 10000
       }
       batch_size=10
    }
  }
}
only_graph_100k = ${main} {
  tasks = {
    graph = ${graph} {
       dataset = ${graph.dataset} {
          mask_link = false
	  max_rows = 100000
       }
    }
  }
}

mtl_iterative_10k = ${main} {
  model_type=BatchIterative
  tasks = {
    graph = ${graph} {
       dataset = ${graph.dataset} {
          mask_link = false
	  max_rows= 10000
       }
       batch_size=10
    }
    coref = ${coref}
  }
}

mtl_iterative_100k = ${main} {
  model_type=BatchIterative
  tasks = {
    graph = ${graph} {
       dataset = ${graph.dataset} {
          mask_link = false
	  max_rows = 100000
       }
    }
    coref = ${coref}
  }
}



tmp = ${main} {
  tasks = {
    coref = ${coref}
  }
}

