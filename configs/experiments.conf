# Word embeddings.
embeddings_dir = "dataset/embeddings"
glove_300d_filtered {
  path = ${embeddings_dir}/glove.840B.300d.txt.filtered
  size = 300
  skip_first = true
}
turian_50d {
  path = ${embeddings_dir}/turian.50d.txt
  size = 50
  skip_first = true
}

# Task dependent configs.
coref {
  name = coref
  dataset = ${conll2012shared}
  model_type=CoreferenceResolution

  # Shared params.
  dropout_rate = ${main.dropout_rate}
  lexical_dropout_rate = ${main.lexical_dropout_rate}

  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  mention_ratio = 0.4

  # Model hyperparameters.
  loss_weight = 1.0 # Fixed. This is for weighted loss of each task in MTL.
  batch_size = 1 # Fixed. Current model can handle only the case batch_size = 1.
  filter_widths = [3, 4, 5]
  filter_size = 50
  ffnn_size = 150
  ffnn_depth = 2
  f_embedding_size = 20
  max_mention_width = 10
  use_metadata = true
  use_features = true
  model_heads = true
}

graph{
  name=graph
  dataset=${wikiP2D}
  model_type=GraphLinkPrediction
}

desc{
  name=desc
  dataset=${wikiP2D}
  model_type=DescriptionGeneration

  decoder_cell = ${main.decoder_cell}
  rnn_size = ${main.rnn_size}
  num_layers = ${main.num_layers}
  dropout_rate = ${main.dropout_rate}
  max_output_len=${wikiP2D.max_sent_len.decode}
  beam_width=5
  length_penalty_weight=0.6 # https://arxiv.org/pdf/1609.08144.pdf
}


# dataset.
wikiP2D{
  name=wikiP2D
  dataset_type=WikiP2DDataset
  data_dir=dataset/wikiP2D
  max_sent_len = {
    encode = 30
    decode = 10
  }
  n_triples = 0
  data_dir = dataset/wikiP2D/source
  filename = Q5O15000R300.half.bin
}
conll2012shared{
  name=conll
  dataset_type=CoNLL2012CorefDataset
  data_dir = dataset/coref/source
  conll_dir = dataset/coref/conll-2012
  train_data = train.english.jsonlines
  train_gold = train.english.v4_auto_conll
  valid_data = dev.english.jsonlines
  valid_gold = dev.english.v4_auto_conll
  test_data = test.english.jsonlines
  test_gold = test.english.v4_gold_conll
}


# Global hyperparameters.
main {
  # Multi-task configuration.
  model_type = MeanLoss
  tasks = [
    ${desc},
  ]
  use_local_rnn = true

  # Shared hyperparameters.
  encoder_cell = GRUCell
  #  encoder_cell = CustomLSTMCell
  decoder_cell = ${main.encoder_cell}
  rnn_size = 200
  num_layers = 1
  dropout_rate = 0.2          # keep_prob = 1.0 - rate
  lexical_dropout_rate = 0.5  # keep_prob = 1.0 - rate

  # Learning hyperparameters.
  max_to_keep = 5
  max_epoch = 30
  max_gradient_norm = 5.0
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  trainable_emb = true

  # Word and character processing.
  cbase = true
  wbase = true
  w_vocab_size = 20000
  c_vocab_size = 1000
  w_embedding_size = 300
  c_embedding_size = 8
  lowercase = true

  #embeddings_conf = [${glove_300d_filtered}, ${turian_50d}]
  embeddings_conf = [${glove_300d_filtered}]
  char_vocab_path = ${embeddings_dir}/char_vocab.english.txt
}
