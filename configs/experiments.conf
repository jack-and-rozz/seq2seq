###################################
#   Embedding initialization
###################################
embeddings_dir = "embeddings"
category_300d {
  path = ${embeddings_dir}/categories.glove.300d.txt
  size = 300
  skip_first = false
}
glove_300d{
  path = ${embeddings_dir}/glove.840B.300d.txt
  size = 300
  skip_first = false
}
glove_300d_desc{
  path = ${embeddings_dir}/glove.840B.300d.txt.for_desc
  size = 300
  skip_first = false
}

# Filtered by all of the CoNLL datasets, including valid and test.
glove_300d_filtered_all_conll {
  path = ${embeddings_dir}/glove.840B.300d.txt.filtered.all_conll
  size = 300
  skip_first = false
}

# Filtered by all of the training data in CoNLL datasets.
glove_300d_filtered_all_train {
  path = ${embeddings_dir}/filtered.conll/glove.840B.300d.txt.all_train
  size = 300
  skip_first = false
}
glove_300d_filtered_wo_bc{
  path = ${embeddings_dir}/filtered.conll/glove.840B.300d.txt.wo_bc
  size = 300
  skip_first = false
}
glove_300d_filtered_wo_bn{
  path = ${embeddings_dir}/filtered.conll/glove.840B.300d.txt.wo_bn
  size = 300
  skip_first = false
}
glove_300d_filtered_wo_mz{
  path = ${embeddings_dir}/filtered.conll/glove.840B.300d.txt.wo_mz
  size = 300
  skip_first = false
}
glove_300d_filtered_wo_nw{
  path = ${embeddings_dir}/filtered.conll/glove.840B.300d.txt.wo_nw
  size = 300
  skip_first = false
}
glove_300d_filtered_wo_pt{
  path = ${embeddings_dir}/filtered.conll/glove.840B.300d.txt.wo_pt
  size = 300
  skip_first = false
}
glove_300d_filtered_wo_tc{
  path = ${embeddings_dir}/filtered.conll/glove.840B.300d.txt.wo_tc
  size = 300
  skip_first = false
}
glove_300d_filtered_wo_wb{
  path = ${embeddings_dir}/filtered.conll/glove.840B.300d.txt.wo_wb
  size = 300
  skip_first = false
}

turian_50d {
  path = ${embeddings_dir}/turian.50d.txt
  size = 50
  skip_first = false
}

# Legacy
glove_300d_filtered {
  path = ${embeddings_dir}/glove.840B.300d.txt.filtered
  size = 300
  skip_first = false
}

glove_300d_filtered_wikipedia {
  path = ${embeddings_dir}/glove.840B.300d.txt.filtered.wikipedia
  size = 300
  skip_first = false
}



###################################
#         Optimizer
###################################

adam {
  optimizer_type = AdamOptimizer
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}
sgd {
  optimizer_type = GradientDescentOptimizer
  learning_rate = 0.1
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}

#################################
#    Task Dependent Models
#################################

coref {
  loss_weight = 1.0
  dataset = ${conll2012shared}
  scorer_path=${conll2012shared.conll_dir}/scorer/v8.01/scorer.pl

  model_type=CoreferenceResolution
  use_local_rnn = ${main.use_local_rnn}
  train_shared = true # Whether to propagate gradients to task-shared layers (RNN, embeddings, etc.) 

  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  mention_ratio = 0.4

  # Model hyperparameters.
  batch_size = 1 # Fixed. Current model can handle only the case batch_size = 1.
  dropout_rate = ${main.dropout_rate}
  ffnn_size = 150
  ffnn_depth = 2
  f_embedding_size = 20
  use_genre_feature = true
  use_speaker_feature = true
  use_width_feature = true
  use_distance_feature = true
  max_mention_width = 10
  # Moved to encoder.
  #use_boundary = true
  #model_heads = true
}

graph{
  loss_weight = 1.0
  dataset = ${wikiP2Dgraph}
  model_type = GraphLinkPrediction
  use_local_rnn = ${main.use_local_rnn}
  dropout_rate = ${main.dropout_rate}
  batch_size = 100
  ffnn_size = 150
  cnn = {
    filter_widths = [2, 3]
    filter_size = 50
  }
}

relex{
  loss_weight = 1.0
  dataset = ${wikiP2Drelex}
  model_type = RelationExtraction
  use_local_rnn = ${main.use_local_rnn}
  mention_ratio = 0.2
  max_mention_width = 5

  # Model hyperparameters.
  batch_size = 1 # Fixed. Current model can handle only the case batch_size = 1.
  dropout_rate = ${main.dropout_rate}
  ffnn_size = 150
  ffnn_depth = 2
  use_width_feature = true
  embedding_size = {
     width = 20
  }
  cnn = {
    filter_widths = [2, 3, 4]
    filter_size = 50
  }
  encode_rel_names = true
  use_gold_mentions = false
  use_predicted_mentions = true
}

desc{
  loss_weight = 1.0
  batch_size = 100
  dataset=${wikiP2Ddesc}
  model_type=DescriptionGeneration
  train_shared = true # Whether to propagate gradients to task-shared layers (RNN, embeddings, etc.) 

  use_local_rnn = ${main.use_local_rnn}
  #entity_word_dropout_rate=1.0
  #context_word_dropout_rate=${main.word_dropout_rate}
  decoder = ${decoder}
  dropout_rate = ${main.dropout_rate}
}

category{
  loss_weight = 1.0
  batch_size = 20
  dataset=${wikiP2Dcategory}
  model_type=CategoryClassification

  use_local_rnn = ${main.use_local_rnn}
  dropout_rate = ${main.dropout_rate}
}


adversarial{
  loss_weight = 1.0
  adv_weight = 1.0 # 0.05
  diff_weight = 1.0 #0.01
  model_type = TaskAdversarial
  use_local_rnn = false
}

#################################
#          Dataset
#################################

# Base setting
wikiP2D {
  minlen = {
     word=0
     char=5
  }
  maxlen = {
     word=0
     char=10
  }
  filename = {
    train = train.jsonlines
    valid = dev.jsonlines
    test  = test.jsonlines
  }
  max_rows = {
     train = 0
     valid = 5000
     test = 5000
  }
}

wikiP2Dgraph = ${wikiP2D} {
  source_dir=dataset/wikiP2D/source/graph
  dataset_type=WikiP2DGraphDataset
  mask_link = false
  prop_data = properties.tokenized.jsonlines
}

wikiP2Drelex = ${wikiP2D}{
  #source_dir=dataset/wikiP2D/source/relex
  source_dir=dataset/wikiP2D/source/relex.hypernym.top50
  dataset_type=WikiP2DRelExDataset
  mask_link = false
  prop_data = properties.jsonlines
  max_mention_width = ${relex.max_mention_width}
  min_triples = 3
  iterations_per_epoch = 2802
}

wikiP2Ddesc = ${wikiP2D} {
  #source_dir=dataset/wikiP2D/source/desc_and_category
  source_dir=dataset/wikiP2D/latest/desc
  dataset_type=WikiP2DDescDataset
  mask_link = false
  max_contexts = 1 # Number of sentences with links per an entity.
  iterations_per_epoch = 2802
}

wikiP2Dcategory = ${wikiP2D} {
  source_dir=dataset/wikiP2D/source/desc_and_category
  dataset_type=WikiP2DCategoryDataset
  max_contexts = 3 # Number of sentences with links per an entity.
  iterations_per_epoch = 2802 # Only to fix the number of sampled examples for training. This is equal to the number of lines on conll2012 training data.

  mask_link = false
  category_vocab = category_freq.txt
  category_size = 500
  embeddings_conf = [${category_300d}] # for category emb initialization.
}

conll2012shared{
  dataset_type = CoNLL2012CorefDataset
  source_dir = dataset/coref/source
  gold_dir = dataset/coref/source
  conll_dir = dataset/coref/conll-2012
  train_data = train.english.jsonlines
  valid_data = dev.english.jsonlines
  test_data = test.english.jsonlines

  train_gold = train.english.v4_auto_conll
  valid_gold = dev.english.v4_auto_conll
  test_gold = test.english.v4_gold_conll
  genres = {
     train = [bc, bn, mz, nw, pt, tc, wb]
     valid = [bc, bn, mz, nw, pt, tc, wb]
     test = [bc, bn, mz, nw, pt, tc, wb]
  }
}


vocab {
  encoder = {
    word = {
      vocab_size = 50000
      trainable = false
      lowercase = false
      normalize_digits = true
      normalize_embedding = true
      #emb_configs = [${glove_300d_filtered_all_conll}, ${turian_50d}]
      emb_configs = [${glove_300d_filtered_all_train}]
    }
    char = {
      vocab_size = 200
      vocab_path = ${embeddings_dir}/char_vocab.english.txt
    }
  }
  decoder = {
    word = {
      vocab_size = 15000
      trainable = true
      lowercase = true
      normalize_digits = true
      normalize_embedding = true
      emb_configs = [${glove_300d_desc}]
    }
  }
}

encoder {
  cell = CustomLSTMCell
  rnn_size = 200
  num_layers = 1
  cnn = {
    filter_widths = [3, 4, 5]
    filter_size = 50
  }

  embedding_size = {
    word = 300
    char = 8
  }
  dropout_rate = ${main.dropout_rate}
  lexical_dropout_rate = 0.5   # for char embeddings encoded by CNN and word embeddings.

  # The functions to merge multiple output states. They must be either concat or reduce_mean.
  merge_func = {
    birnn = reduce_mean  # For fw/bw RNN. 
    # Only for MTL using Multi-RNN (Shared/Private).
    shared_private = concat # For outputs from shared/private RNN. It has to be concat when using adversarial learning for encoder's outputs, so as to obtain shared and private represations respectively. 
    mentions = reduce_mean   # For mentions aggregated by shared/private RNN.
  }
  # For mention representations.
  use_boundary = true
  model_heads = true
}

decoder {
  cell = GRUCell #${encoder.cell}
  rnn_size = ${encoder.rnn_size}
  num_layers = ${encoder.num_layers}
  max_output_len = 10
  beam_width=5
  length_penalty_weight=0.6 # https://arxiv.org/pdf/1609.08144.pdf
  dropout_rate = ${main.dropout_rate}
}

########################################################

#################################
#          Main 
#################################
main {
  # Multi-task learning schema
  model_type = MeanLoss #BatchIterative

  # These tasks must be in order of their importances to decide training orders and the way of choosing the best model. Moreover, the adversarial task has to be on the last of this list.
  tasks = {
    #coref = ${coref}
    #graph = ${graph}
  }

  # About encoder
  encoder = ${encoder} 
  use_local_rnn = false

  # Learning hyperparameters
  max_to_keep = 1
  max_epoch = 120
  dropout_rate = 0.2   # keep_prob = 1.0 - dropout_rate
  word_dropout_rate = 0.0
  optimizer = ${adam}

  # Word and character processings
  vocab = ${vocab}
}


tmp = ${main} {
  model_type = GradientSum
  max_epoch = 2
  tasks = {
    #coref = ${coref}
    # coref = ${coref_wo_genre}{
    #     dataset = ${conll2012shared_wo_bn}
    # }
    desc = ${desc}{
      batch_size = 1
      dataset = ${desc.dataset}{
        max_rows = {
    	  train = 2000
    	  valid = 1
    	  test = 1
    	}
    	iterations_per_epoch = 1000
      }
    }

    # #coref = ${coref}
    # relex = ${relex}{
    #   use_gold_mentions = true
    #   use_predicted_mentions = true
    #   batch_size = 20
    #   dataset = ${relex.dataset}{
    #     max_rows = {
    # 	  train = 1000
    # 	  dev = 300
    # 	  test = 300
    # 	}
    #   }
    # }
    # category = ${category} {
    #   batch_size = 2
    #   dataset = ${category.dataset}{
    #      max_rows = 2000
    #   }
    # }
    #adversarial=${adversarial}
    # graph = ${graph} {
    #    dataset = ${wikiP2Dgraph}{
    #       max_rows = 100
    #    }
    # }
  }
}



##################################################3
#                 Variants
##################################################3

coref_base = ${main} {
  tasks = {
    coref = ${coref}
  }
}

desc_base = ${main} {
  tasks = {
    desc = ${desc}
  }
}

mtl_meanloss = ${main} {
  tasks = {
    coref = ${coref}
    desc = ${desc}
  }
  model_type = MeanLoss
}

mtl_separately = ${main} {
  tasks = {
    coref = ${coref}
    desc = ${desc}
  }
  model_type = SeparatelyUpdate
}

mtl_gradientsum = ${main} {
  tasks = {
    coref = ${coref}
    desc = ${desc}
  }
  model_type = GradientSum
}


mtl_meanloss_local = ${main} {
  use_local_rnn = true
  tasks = {
    coref = ${coref} {
      use_local_rnn = true
    }
    desc = ${desc} {
      use_local_rnn = true
    }
  }
  model_type = MeanLoss
}

mtl_iterative = ${main} {
  tasks = {
    coref = ${coref}
    desc = ${desc}
  }
  model_type = BatchIterative
}

mtl_noprop = ${main} {
  tasks = {
    coref = ${coref}
    desc = ${desc}{
      train_shared = false
    }
  }
  model_type = SumLoss
}

mtl_adv = ${main} {
  use_local_rnn = true
  tasks = {
    coref = ${coref} {
      use_local_rnn = true
    }
    desc = ${desc} {
      use_local_rnn = true
    }
    adv = ${adversarial}
  }
  model_type = MeanLoss
}



#########################################
##        Ablation Test
#########################################

vocab_wo_bc = ${vocab}{
  encoder = {
    word = {
      emb_configs = [${glove_300d_filtered_wo_bc}]
    }
  }
}
vocab_wo_bn = ${vocab}{
  encoder = {
    word = {
      emb_configs = [${glove_300d_filtered_wo_bn}]
    }
  }
}
vocab_wo_mz = ${vocab}{
  encoder = {
    word = {
      emb_configs = [${glove_300d_filtered_wo_mz}]
    }
  }
}
vocab_wo_nw = ${vocab}{
  encoder = {
    word = {
      emb_configs = [${glove_300d_filtered_wo_nw}]
    }
  }
}
vocab_wo_pt = ${vocab}{
  encoder = {
    word = {
      emb_configs = [${glove_300d_filtered_wo_pt}]
    }
  }
}
vocab_wo_tc = ${vocab}{
  encoder = {
    word = {
      emb_configs = [${glove_300d_filtered_wo_tc}]
    }
  }
}
vocab_wo_wb = ${vocab}{
  encoder = {
    word = {
      emb_configs = [${glove_300d_filtered_wo_wb}]
    }
  }
}

conll2012shared_wo_bc = ${conll2012shared}{
  gold_dir = dataset/coref/source/bc
  genres = {
     train = [bn, mz, nw, pt, tc, wb]
     valid = [bc]
     test = [bc]
  }
}
conll2012shared_wo_bn = ${conll2012shared}{
  gold_dir = dataset/coref/source/bn
  genres = {
     train = [bc, mz, nw, pt, tc, wb]
     valid = [bn]
     test = [bn]
  }
}
conll2012shared_wo_mz = ${conll2012shared}{
  gold_dir = dataset/coref/source/mz
  genres = {
     train = [bc, bn, nw, pt, tc, wb]
     valid = [mz]
     test = [mz]
  }
}
conll2012shared_wo_nw = ${conll2012shared}{
  gold_dir = dataset/coref/source/nw
  genres = {
     train = [bc, bn, mz, pt, tc, wb]
     valid = [nw]
     test = [nw]
  }
}
conll2012shared_wo_pt = ${conll2012shared}{
  gold_dir = dataset/coref/source/pt
  genres = {
     train = [bc, bn, mz, nw, tc, wb]
     valid = [pt]
     test = [pt]
  }
}
conll2012shared_wo_tc = ${conll2012shared}{
  gold_dir = dataset/coref/source/tc
  genres = {
     train = [bc, bn, mz, nw, pt, wb]
     valid = [tc]
     test = [tc]
  }
}
conll2012shared_wo_wb = ${conll2012shared}{
  gold_dir = dataset/coref/source/wb
  genres = {
     train = [bc, bn, mz, nw, pt, tc]
     valid = [wb]
     test = [wb]
  }
}
coref_wo_genre = ${coref}{
  use_genre_feature = false
}

##############

# 1-AB
coref_base_wo_bc = ${coref_base}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_bc}
    }
  }
  vocab = ${vocab_wo_bc}
}

mtl_gradientsum_wo_bc = ${mtl_gradientsum}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_bc}
    }
    desc = ${desc}
  }
  vocab = ${vocab_wo_bc}
}

# 2-AB
coref_base_wo_bn = ${coref_base}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_bn}
    }
  }
  vocab = ${vocab_wo_bn}
}

mtl_gradientsum_wo_bn = ${mtl_gradientsum}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_bn}
    }
    desc = ${desc}
  }
  vocab = ${vocab_wo_bn}
}

# 3-AB
coref_base_wo_mz = ${coref_base}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_mz}
    }
  }
  vocab = ${vocab_wo_mz}
}

mtl_gradientsum_wo_mz = ${mtl_gradientsum}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_mz}
    }
    desc = ${desc}
  }
  vocab = ${vocab_wo_mz}
}

# 4-AB
coref_base_wo_nw = ${coref_base}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_nw}
    }
  }
  vocab = ${vocab_wo_nw}
}

mtl_gradientsum_wo_nw = ${mtl_gradientsum}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_nw}
    }
    desc = ${desc}
  }
  vocab = ${vocab_wo_nw}
}

# 5-AB
coref_base_wo_pt = ${coref_base}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_pt}
    }
  }
  vocab = ${vocab_wo_pt}
}

mtl_gradientsum_wo_pt = ${mtl_gradientsum}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_pt}
    }
    desc = ${desc}
  }
  vocab = ${vocab_wo_pt}
}

# 6-AB
coref_base_wo_tc = ${coref_base}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_tc}
    }
  }
  vocab = ${vocab_wo_tc}
}

mtl_gradientsum_wo_tc = ${mtl_gradientsum}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_tc}
    }
    desc = ${desc}
  }
  vocab = ${vocab_wo_tc}
}

# 7-AB
coref_base_wo_wb = ${coref_base}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_wb}
    }
  }
  vocab = ${vocab_wo_wb}
}

mtl_gradientsum_wo_wb = ${mtl_gradientsum}{
  tasks = {
    coref = ${coref_wo_genre}{
      dataset = ${conll2012shared_wo_wb}
    }
    desc = ${desc}
  }
  vocab = ${vocab_wo_wb}
}



##########################################
##             Unused
##########################################

mtl_onebyone = ${main} {
  tasks = {
    coref = ${coref}
    desc = ${desc}
  }
  model_type = OneByOne
}


graph_base = ${main} {
  tasks = {
    graph = ${graph}
  }
}

category_base = ${main}{
  tasks = {
    category = ${category}
  }
}

relex_base = ${main}{
  batch_size = 80
  lowercase = false
  normalize_digits = true
  embeddings_conf = [${glove_300d_filtered_wikipedia}]#, ${turian_50d}]
  tasks = {
    relex = ${relex}{
      dataset = ${relex.dataset}{
        max_rows = {
	  train = 0
	  valid = 0
	  test = 0
	}
	iterations_per_epoch = 500
      }
      use_gold_mentions = false #true
      use_predicted_mentions = true
    }
  }
}
