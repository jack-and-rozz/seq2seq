###################################
#   Embedding initialization
###################################
embeddings_dir = "embeddings"
category_300d {
  path = ${embeddings_dir}/categories.glove.300d.txt
  size = 300
  skip_first = false
}
glove_300d{
  path = ${embeddings_dir}/glove.840B.300d.txt
  size = 300
  skip_first = false
}
glove_300d_filtered {
  path = ${embeddings_dir}/glove.840B.300d.txt.filtered
  size = 300
  skip_first = false
}
turian_50d {
  path = ${embeddings_dir}/turian.50d.txt
  size = 50
  skip_first = false
}

###################################
#         Optimizer
###################################

adam {
  optimizer_type = AdamOptimizer
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}
sgd {
  optimizer_type = GradientDescentOptimizer
  learning_rate = 0.1
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}

#################################
#    Task Dependent Models
#################################

coref {
  loss_weight = 1.0
  dataset = ${conll2012shared}
  model_type=CoreferenceResolution
  use_local_rnn = ${main.use_local_rnn}

  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  mention_ratio = 0.3

  # Model hyperparameters.
  batch_size = 1 # Fixed. Current model can handle only the case batch_size = 1.
  dropout_rate = ${main.dropout_rate}
  ffnn_size = 150
  ffnn_depth = 2
  f_embedding_size = 20
  use_metadata = true
  use_width_feature = true
  use_distance_feature = true
  max_mention_width = 10
  # Moved to encoder.
  #use_boundary = true
  #model_heads = true
}

graph{
  loss_weight = 1.0
  dataset = ${wikiP2Dgraph}
  model_type = GraphLinkPrediction
  use_local_rnn = ${main.use_local_rnn}
  dropout_rate = ${main.dropout_rate}
  batch_size = 100
  ffnn_size = 150
  cnn = {
    filter_widths = [2, 3]
    filter_size = 50
  }
}

relex{
  loss_weight = 1.0
  dataset = ${wikiP2Drelex}
  model_type=GraphLinkPredictionNoObj
  use_local_rnn = ${main.use_local_rnn}
  mention_ratio = 0.4
  max_mention_width = 5

  # Model hyperparameters.
  batch_size = 1 # Fixed. Current model can handle only the case batch_size = 1.
  dropout_rate = ${main.dropout_rate}
  ffnn_size = 150
  ffnn_depth = 2
  use_width_feature = true
  embedding_size = {
     width = 20
  }
}

desc{
  loss_weight=1.0
  batch_size = 5
  dataset=${wikiP2Ddesc}
  model_type=DescriptionGeneration

  use_local_rnn = ${main.use_local_rnn}
  entity_word_dropout_rate=1.0
  context_word_dropout_rate=${main.word_dropout_rate}
  decoder = ${main.encoder}
  dropout_rate = ${main.dropout_rate}
  beam_width=5
  length_penalty_weight=0.6 # https://arxiv.org/pdf/1609.08144.pdf
}

category{
  loss_weight=1.0
  batch_size = 20
  dataset=${wikiP2Dcategory}
  model_type=CategoryClassification

  use_local_rnn = ${main.use_local_rnn}
  dropout_rate = ${main.dropout_rate}
}


adversarial{
  loss_weight = 1.0
  adv_weight = 0.05
  diff_weight = 0.01
  model_type = TaskAdversarial
  use_local_rnn = false
}

#################################
#          Dataset
#################################

# Base setting
wikiP2D {
  minlen = {
     word=0
     char=5
  }
  maxlen = {
     word=0
     char=10
  }
  filename = {
    train = train.jsonlines
    valid = dev.jsonlines
    test  = test.jsonlines
  }
  max_rows = {
     train = 0
     valid = 0
     test = 0
  }
}

wikiP2Dgraph = ${wikiP2D} {
  source_dir=dataset/wikiP2D/source/graph
  dataset_type=WikiP2DGraphDataset
  mask_link = false
  prop_data = properties.tokenized.jsonlines
}

wikiP2Drelex = ${wikiP2D}{
  source_dir=dataset/wikiP2D/source/relex
  dataset_type=WikiP2DRelExDataset
  mask_link = false
  prop_data = properties.jsonlines
  max_mention_width = ${relex.max_mention_width}
  min_triples = 3
  iterations_per_epoch = 0 # Only to fix the number of sampled examples for training. 
}

wikiP2Ddesc = ${wikiP2D} {
  source_dir=dataset/wikiP2D/source/desc_and_category
  dataset_type=WikiP2DDescDataset
  mask_link = false
  max_contexts = 3 # Number of sentences with links per an entity.
}

wikiP2Dcategory = ${wikiP2D} {
  source_dir=dataset/wikiP2D/source/desc_and_category
  dataset_type=WikiP2DCategoryDataset
  max_contexts = 3 # Number of sentences with links per an entity.
  iterations_per_epoch = 2802 # Only to fix the number of sampled examples for training. This is equal to the number of lines on conll2012 training data.

  mask_link = false
  category_vocab = category_freq.txt
  category_size = 500
  embeddings_conf = [${category_300d}] # for category emb initialization.
}

conll2012shared{
  dataset_type=CoNLL2012CorefDataset
  source_dir = dataset/coref/source
  conll_dir = dataset/coref/conll-2012
  train_data = train.english.jsonlines
  train_gold = train.english.v4_auto_conll
  valid_data = dev.english.jsonlines
  valid_gold = dev.english.v4_auto_conll
  test_data = test.english.jsonlines
  test_gold = test.english.v4_gold_conll
}

encoder {
  cell = CustomLSTMCell
  trainable_emb = false
  rnn_size = 200
  num_layers = 1
  vocab_size = ${main.vocab_size}
  cnn = {
    filter_widths = [3, 4, 5]
    filter_size = 50
  }

  embedding_size = {
    word = 300
    char = 8
  }
  dropout_rate = ${main.dropout_rate}
  lexical_dropout_rate = 0.5   # for char embeddings encoded by CNN and word embeddings.

  # The functions to merge multiple output states. They must be either concat or reduce_mean.
  merge_func = {
    birnn = reduce_mean  # For fw/bw RNN. 
    # Only for MTL using Multi-RNN (Shared/Private).
    shared_private = reduce_mean # For outputs from shared/private RNN.
    mentions = reduce_mean   # For mentions aggregated by shared/private RNN.
  }
  # For mention representations.
  use_boundary = true
  model_heads = true
}


########################################################

#################################
#          Main 
#################################
main {
  # Multi-task learning schema
  model_type = BatchIterative
  
  # These tasks must be in order of their importances to decide training orders and the way of choosing the best model. Moreover, the adversarial task has to be on the last of this list.
  tasks = {
    #coref = ${coref}
    #graph = ${graph}
  }

  # About encoder
  encoder = ${encoder} 
  use_local_rnn = false

  # Learning hyperparameters
  max_to_keep = 1
  max_epoch = 80
  dropout_rate = 0.2   # keep_prob = 1.0 - dropout_rate
  word_dropout_rate = 0.0
  optimizer = ${adam}

  # Word and character processings
  vocab_size = { # if the corresponding size is 0, the token type is not used.
    word = 30000 
    char = 200
  }
  lowercase = false
  normalize_digits = true
  embeddings_conf = [${glove_300d_filtered}, ${turian_50d}]
  char_vocab_path = ${embeddings_dir}/char_vocab.english.txt
}


tmp = ${main} {
  model_type = BatchIterative
  #  model_type = MeanLoss
  tasks = {
    #coref = ${coref}
    #desc = ${desc}{
    #  max_rows = 100
    #}
    #coref = ${coref}
    relex = ${relex}{
      dataset = ${relex.dataset}{
        max_rows = {
	  train = 10000
	  dev = 30
	  test = 200
	}
      }
    }
    # category = ${category} {
    #   batch_size = 2
    #   dataset = ${category.dataset}{
    #      max_rows = 2000
    #   }
    # }
    #adversarial=${adversarial}
    # graph = ${graph} {
    #    dataset = ${wikiP2Dgraph}{
    #       max_rows = 100
    #    }
    # }
  }
  vocab_size = {
    word = 5000
    char = 100
  }
  encoder = ${encoder}{
    rnn_size = 10
  }
}



##################################################3
#                 Variants
##################################################3

coref_base = ${main} {
  tasks = {
    coref = ${coref}
  }
}

coref_base_h400 = ${main} {
  encoder = ${encoder}{
    rnn_size = 400
  }
  tasks = {
    coref = ${coref}
  }
}

graph_base = ${main} {
  tasks = {
    graph = ${graph}
  }
}

desc_base = ${main} {
  tasks = {
    desc = ${desc}
  }
}

category_base = ${main}{
  tasks = {
    category = ${category}
  }
}
relex_base = ${main}{
  lowercase = true
  normalize_digits = true
  embeddings_conf = [${glove_300d}, ${turian_50d}]
  tasks = {
    relex = ${relex}{
      dataset = ${relex.dataset}{
        max_rows = {
	  train = 0
	  valid = 0
	  test = 0
	}
	iterations_per_epoch = 10000
      }
    }
  }
}

mtl_iterative = ${main} {
  tasks = {
    coref = ${coref}
    category = ${category}
    #category = ${category}
  }
  model_type = BatchIterative
}

mtl_meanloss = ${main} {
  tasks = {
    coref = ${coref}
    category = ${category}
  }
  model_type = MeanLoss
}

mtl_adv = ${main} {
  tasks = {
    coref = ${coref}
    category = ${category}
    adv = ${adversarial}
  }
  model_type = MeanLoss
}


mtl_adv_local = ${main} {
  use_local_rnn = true
  tasks = {
    coref = ${coref} {
      use_local_rnn = true
    }
    category = ${category} {
      use_local_rnn = true
    }
    adv = ${adversarial}
  }
  model_type = MeanLoss
}


mtl_onebyone = ${main} {
  tasks = {
    coref = ${coref}
    category = ${category}
  }
  model_type = OneByOne
}

mtl_ewc = ${main} {
  tasks = {
    coref = ${coref}
    category = ${category}
  }
  model_type = EWC
}






##########################################
##               Legacy
##########################################

