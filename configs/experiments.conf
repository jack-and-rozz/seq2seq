###################################
#   Embedding initialization
###################################
embeddings_dir = "embeddings"
category_300d {
  path = ${embeddings_dir}/categories.glove.300d.txt
  size = 300
  skip_first = false
}
glove_300d{
  path = ${embeddings_dir}/glove.840B.300d.txt
  size = 300
  skip_first = false
}
glove_300d_filtered {
  path = ${embeddings_dir}/glove.840B.300d.txt.filtered
  size = 300
  skip_first = false
}
glove_300d_filtered_conll {
  path = ${embeddings_dir}/glove.840B.300d.txt.filtered.conll
  size = 300
  skip_first = false
}
glove_300d_filtered_wikipedia {
  path = ${embeddings_dir}/glove.840B.300d.txt.filtered.wikipedia
  size = 300
  skip_first = false
}
turian_50d {
  path = ${embeddings_dir}/turian.50d.txt
  size = 50
  skip_first = false
}

###################################
#         Optimizer
###################################

adam {
  optimizer_type = AdamOptimizer
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}
sgd {
  optimizer_type = GradientDescentOptimizer
  learning_rate = 0.1
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}

#################################
#    Task Dependent Models
#################################

coref {
  loss_weight = 1.0
  dataset = ${conll2012shared}
  model_type=CoreferenceResolution
  use_local_rnn = ${main.use_local_rnn}
  train_shared = true # Whether to propagate gradients to task-shared layers (RNN, embeddings, etc.) 

  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  mention_ratio = 0.4

  # Model hyperparameters.
  batch_size = 1 # Fixed. Current model can handle only the case batch_size = 1.
  dropout_rate = ${main.dropout_rate}
  ffnn_size = 150
  ffnn_depth = 2
  f_embedding_size = 20
  use_metadata = true
  use_width_feature = true
  use_distance_feature = true
  max_mention_width = 10
  # Moved to encoder.
  #use_boundary = true
  #model_heads = true
}

graph{
  loss_weight = 1.0
  dataset = ${wikiP2Dgraph}
  model_type = GraphLinkPrediction
  use_local_rnn = ${main.use_local_rnn}
  dropout_rate = ${main.dropout_rate}
  batch_size = 100
  ffnn_size = 150
  cnn = {
    filter_widths = [2, 3]
    filter_size = 50
  }
}

relex{
  loss_weight = 1.0
  dataset = ${wikiP2Drelex}
  model_type = RelationExtraction
  use_local_rnn = ${main.use_local_rnn}
  mention_ratio = 0.2
  max_mention_width = 5

  # Model hyperparameters.
  batch_size = 1 # Fixed. Current model can handle only the case batch_size = 1.
  dropout_rate = ${main.dropout_rate}
  ffnn_size = 150
  ffnn_depth = 2
  use_width_feature = true
  embedding_size = {
     width = 20
  }
  cnn = {
    filter_widths = [2, 3, 4]
    filter_size = 50
  }
  encode_rel_names = true
  use_gold_mentions = false
  use_predicted_mentions = true
}

desc{
  loss_weight = 1.0
  batch_size = 100
  dataset=${wikiP2Ddesc}
  model_type=DescriptionGeneration
  train_shared = true # Whether to propagate gradients to task-shared layers (RNN, embeddings, etc.) 

  use_local_rnn = ${main.use_local_rnn}
  entity_word_dropout_rate=1.0
  context_word_dropout_rate=${main.word_dropout_rate}
  decoder = ${decoder}
  dropout_rate = ${main.dropout_rate}
}

category{
  loss_weight=1.0
  batch_size = 20
  dataset=${wikiP2Dcategory}
  model_type=CategoryClassification

  use_local_rnn = ${main.use_local_rnn}
  dropout_rate = ${main.dropout_rate}
}


adversarial{
  loss_weight = 1.0
  adv_weight = 0.05
  diff_weight = 0.01
  model_type = TaskAdversarial
  use_local_rnn = false
}

#################################
#          Dataset
#################################

# Base setting
wikiP2D {
  minlen = {
     word=0
     char=5
  }
  maxlen = {
     word=0
     char=10
  }
  filename = {
    train = train.jsonlines
    valid = dev.jsonlines
    test  = test.jsonlines
  }
  max_rows = {
     train = 0
     valid = 0
     test = 0
  }
}

wikiP2Dgraph = ${wikiP2D} {
  source_dir=dataset/wikiP2D/source/graph
  dataset_type=WikiP2DGraphDataset
  mask_link = false
  prop_data = properties.tokenized.jsonlines
}

wikiP2Drelex = ${wikiP2D}{
  #source_dir=dataset/wikiP2D/source/relex
  source_dir=dataset/wikiP2D/source/relex.hypernym.top50
  dataset_type=WikiP2DRelExDataset
  mask_link = false
  prop_data = properties.jsonlines
  max_mention_width = ${relex.max_mention_width}
  min_triples = 3
  iterations_per_epoch = 0 
}

wikiP2Ddesc = ${wikiP2D} {
  source_dir=dataset/wikiP2D/source/desc_and_category
  dataset_type=WikiP2DDescDataset
  mask_link = false
  max_contexts = 1 # Number of sentences with links per an entity.
  iterations_per_epoch = 0 
}

wikiP2Dcategory = ${wikiP2D} {
  source_dir=dataset/wikiP2D/source/desc_and_category
  dataset_type=WikiP2DCategoryDataset
  max_contexts = 3 # Number of sentences with links per an entity.
  iterations_per_epoch = 2802 # Only to fix the number of sampled examples for training. This is equal to the number of lines on conll2012 training data.

  mask_link = false
  category_vocab = category_freq.txt
  category_size = 500
  embeddings_conf = [${category_300d}] # for category emb initialization.
}

conll2012shared{
  dataset_type=CoNLL2012CorefDataset
  source_dir = dataset/coref/source
  conll_dir = dataset/coref/conll-2012
  train_data = train.english.jsonlines
  train_gold = train.english.v4_auto_conll
  valid_data = dev.english.jsonlines
  valid_gold = dev.english.v4_auto_conll
  test_data = test.english.jsonlines
  test_gold = test.english.v4_gold_conll
}

encoder {
  cell = CustomLSTMCell
  trainable_emb = false
  rnn_size = 200
  num_layers = 1
  wbase = true
  cbase = true
  cnn = {
    filter_widths = [3, 4, 5]
    filter_size = 50
  }

  embedding_size = {
    word = 300
    char = 8
  }
  dropout_rate = ${main.dropout_rate}
  lexical_dropout_rate = 0.5   # for char embeddings encoded by CNN and word embeddings.

  # The functions to merge multiple output states. They must be either concat or reduce_mean.
  merge_func = {
    birnn = reduce_mean  # For fw/bw RNN. 
    # Only for MTL using Multi-RNN (Shared/Private).
    shared_private = concat # For outputs from shared/private RNN. It has to be concat when using adversarial learning for encoder's outputs, so as to obtain shared and private represations respectively. 
    mentions = reduce_mean   # For mentions aggregated by shared/private RNN.
  }
  # For mention representations.
  use_boundary = true
  model_heads = true
}

decoder = {
  cell = GRUCell #${encoder.cell}
  #rnn_size = ${encoder.rnn_size}
  num_layers = ${encoder.num_layers}
  max_output_len = 10
  beam_width=5
  length_penalty_weight=0.6 # https://arxiv.org/pdf/1609.08144.pdf
}

########################################################

#################################
#          Main 
#################################
main {
  # Multi-task learning schema
  model_type = BatchIterative
  
  # These tasks must be in order of their importances to decide training orders and the way of choosing the best model. Moreover, the adversarial task has to be on the last of this list.
  tasks = {
    #coref = ${coref}
    #graph = ${graph}
  }

  # About encoder
  encoder = ${encoder} 
  use_local_rnn = false

  # Learning hyperparameters
  max_to_keep = 1
  max_epoch = 80
  dropout_rate = 0.2   # keep_prob = 1.0 - dropout_rate
  word_dropout_rate = 0.0
  optimizer = ${adam}

  # Word and character processings
  vocab_size = { # if the corresponding size is 0, the token type is not used.
    word = 30000 
    char = 200
  }
  lowercase = false
  normalize_digits = true
  embeddings_conf = [${glove_300d_filtered}, ${turian_50d}]
  char_vocab_path = ${embeddings_dir}/char_vocab.english.txt
}


tmp = ${main} {
  #model_type = BatchIterative
  model_type = MeanLoss
  tasks = {
    #coref = ${coref}
    desc = ${desc}{
      batch_size = 2
      dataset = ${desc.dataset}{
        max_rows = {
	  train =1
	  valid = 1
	  test = 1
	}
      }
    }
    # #coref = ${coref}
    # relex = ${relex}{
    #   use_gold_mentions = true
    #   use_predicted_mentions = true
    #   batch_size = 20
    #   dataset = ${relex.dataset}{
    #     max_rows = {
    # 	  train = 1000
    # 	  dev = 300
    # 	  test = 300
    # 	}
    #   }
    # }
    # category = ${category} {
    #   batch_size = 2
    #   dataset = ${category.dataset}{
    #      max_rows = 2000
    #   }
    # }
    #adversarial=${adversarial}
    # graph = ${graph} {
    #    dataset = ${wikiP2Dgraph}{
    #       max_rows = 100
    #    }
    # }
  }
  vocab_size = {
    word = 5000
    char = 100
  }
  encoder = ${encoder}{
    rnn_size = 10
  }
}



##################################################3
#                 Variants
##################################################3

coref_base = ${main} {
  tasks = {
    coref = ${coref}
  }
}

coref_base_h400 = ${main} {
  encoder = ${encoder}{
    rnn_size = 400
  }
  tasks = {
    coref = ${coref}
  }
}

graph_base = ${main} {
  tasks = {
    graph = ${graph}
  }
}

desc_base = ${main} {
  tasks = {
    desc = ${desc}
  }
}

category_base = ${main}{
  tasks = {
    category = ${category}
  }
}
relex_base = ${main}{
  batch_size = 80
  lowercase = false
  normalize_digits = true
  embeddings_conf = [${glove_300d_filtered_wikipedia}]#, ${turian_50d}]
  tasks = {
    relex = ${relex}{
      dataset = ${relex.dataset}{
        max_rows = {
	  train = 0
	  valid = 0
	  test = 0
	}
	iterations_per_epoch = 500
      }
      use_gold_mentions = false #true
      use_predicted_mentions = true
    }
  }
}

mtl_iterative = ${main} {
  tasks = {
    coref = ${coref}
    desc = ${desc}
  }
  model_type = BatchIterative
}

mtl_meanloss = ${main} {
  tasks = {
    coref = ${coref}
    desc = ${desc}
  }
  model_type = MeanLoss
}

mtl_adv = ${main} {
  tasks = {
    coref = ${coref}
    category = ${desc}
    adv = ${adversarial}
  }
  model_type = MeanLoss
}


mtl_adv_local = ${main} {
  use_local_rnn = true
  tasks = {
    coref = ${coref} {
      use_local_rnn = true
    }
    desc = ${desc} {
      use_local_rnn = true
    }
    adv = ${adversarial}
  }
  model_type = MeanLoss
}


mtl_onebyone = ${main} {
  tasks = {
    coref = ${coref}
    desc = ${desc}
  }
  model_type = OneByOne
}


##########################################
##               Legacy
##########################################

