###################################
#   Embedding initialization
###################################
embeddings_dir = "embeddings"
glove_300d_filtered {
  path = ${embeddings_dir}/glove.840B.300d.txt.filtered
  size = 300
  skip_first = false
}
turian_50d {
  path = ${embeddings_dir}/turian.50d.txt
  size = 50
  skip_first = false
}

###################################
#         Optimizer
###################################

adam {
  optimizer_type = AdamOptimizer
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}
sgd {
  optimizer_type = GradientDescentOptimizer
  learning_rate = 0.1
  decay_rate = 0.999
  decay_frequency = 100
  max_gradient_norm = 5.0
}

#################################
#    Task Dependent Models
#################################

coref {
  loss_weight = 1.0
  dataset = ${conll2012shared}
  model_type=CoreferenceResolution
  use_local_rnn = ${main.use_local_rnn}

  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  mention_ratio = 0.4

  # Model hyperparameters.
  batch_size = 1 # Fixed. Current model can handle only the case batch_size = 1.
  dropout_rate = ${main.dropout_rate}
  filter_widths = [3, 4, 5]
  filter_size = 50
  ffnn_size = 150
  ffnn_depth = 2
  f_embedding_size = 20
  max_mention_width = 10
  use_metadata = true
  use_features = true
  use_boundary = true
  model_heads = true
}

graph{
  loss_weight = 1.0
  dataset = ${wikiP2Dgraph}
  model_type = GraphLinkPrediction
  use_local_rnn = ${main.use_local_rnn}
  dropout_rate = ${main.dropout_rate}
  batch_size = 100
  ffnn_size = 150
  cnn = {
    filter_widths = [2, 3]
    filter_size = 50
  }
}

desc{
  loss_weight=1.0
  batch_size = 1
  dataset=${wikiP2Ddesc}
  model_type=DescriptionGeneration
  entity_word_dropout_rate=1.0
  context_word_dropout_rate=${main.word_dropout_rate}
  decoder_cell = ${main.decoder_cell}
  rnn_size = ${main.rnn_size}
  num_layers = ${main.num_layers}
  dropout_rate = ${main.dropout_rate}
  beam_width=5
  length_penalty_weight=0.6 # https://arxiv.org/pdf/1609.08144.pdf
}
adversarial{
  loss_weight= 1.0
  model_type = TaskAdversarial
  use_local_rnn = false
}

#################################
#          Dataset
#################################

# Base setting
wikiP2D {
  source_dir=dataset/wikiP2D/source
  processed_dir=dataset/wikiP2D/processed
  minlen = {
     word=0
     char=5
  }
  maxlen = {
     word=0
     char=10
  }
  train_data = train.jsonlines
  valid_data = dev.jsonlines
  test_data = test.jsonlines
  max_rows = 100000
}

wikiP2Dgraph = ${wikiP2D} {
  dataset_type=WikiP2DGraphDataset
  mask_link = false
  prop_data = properties.tokenized.jsonlines
}

wikiP2Ddesc = ${wikiP2D} {
  dataset_type=WikiP2DDescDataset
}

conll2012shared{
  dataset_type=CoNLL2012CorefDataset
  source_dir = dataset/coref/source
  conll_dir = dataset/coref/conll-2012
  train_data = train.english.jsonlines
  train_gold = train.english.v4_auto_conll
  valid_data = dev.english.jsonlines
  valid_gold = dev.english.v4_auto_conll
  test_data = test.english.jsonlines
  test_gold = test.english.v4_gold_conll
}

########################################################

#################################
#          Main 
#################################
main {
  # Multi-task configuration.
  model_type = BatchIterative
  tasks = {
    #coref = ${coref}
    #graph = ${graph}
  }

  # Shared hyperparameters.
  encoder_cell = CustomLSTMCell # [GRUCell or CustomLSTMCell]
  decoder_cell = ${main.encoder_cell}
  rnn_size = 200
  num_layers = 1
  use_local_rnn = false

  # Learning hyperparameters.
  max_to_keep = 1
  max_epoch = 40
  dropout_rate = 0.2   # keep_prob = 1.0 - dropout_rate
  lexical_dropout_rate = 0.5   # for word embeddings and char embeddings encoded by CNN.
  word_dropout_rate = 0.0
  optimizer = ${adam}

  # Word and character processing.
  vocab_size = { # if the corresponding size is 0, the token type is not used.
    word = 30000 
    char = 200
  }
  w_embedding_size = 300
  c_embedding_size = 8
  lowercase = false
  normalize_digits = true
  trainable_emb = false
  embeddings_conf = [${glove_300d_filtered}, ${turian_50d}]
  #embeddings_conf = [${glove_300d_filtered}]
  char_vocab_path = ${embeddings_dir}/char_vocab.english.txt
}


tmp = ${main} {
  tasks = {
    coref = ${coref}
  }
  vocab_size = {
    word = 100
    char = 100
  }
  rnn_size = 10
}



##################################################3
#                 Variants
##################################################3

only_coref = ${main} {
  tasks = {
    coref = ${coref}
  }
}

only_graph = ${main} {
  tasks = {
    graph = ${graph}
  }
}

only_desc = ${main} {
  tasks = {
    desc = ${desc}
  }
}

mtl_iterative = ${main} {
  tasks = {
    coref = ${coref}
    graph = ${graph}
  }
  model_type = BatchIterative
}

mtl_meanloss = ${main} {
  tasks = {
    coref = ${coref}
    graph = ${graph}
  }
  model_type = MeanLoss
}

mtl_adv = ${main} {
  tasks = {
    coref = ${coref}
    graph = ${graph}
    adv = ${adversarial}
  }
  model_type = MeanLoss
}


mtl_adv_local = ${main} {
  use_local_rnn = true
  tasks = {
    coref = ${coref} {
      use_local_rnn = true
    }
    graph = ${graph} {
      use_local_rnn = true
    }
    adv = ${adversarial}
  }
  model_type = MeanLoss
}


mtl_onebyone = ${main} {
  tasks = {
    coref = ${coref}
    graph = ${graph}
  }
  model_type = OneByOne
}

mtl_ewc = ${main} {
  tasks = {
    coref = ${coref}
    graph = ${graph}
  }
  model_type = EWC
}






##########################################
##               Legacy
##########################################

