# Word embeddings.
embeddings_dir = "dataset/embeddings"
glove_300d_filtered {
  path = ${embeddings_dir}/glove.840B.300d.txt.filtered
  size = 300
  skip_first = true
}
turian_50d {
  path = ${embeddings_dir}/turian.50d.txt
  size = 50
  skip_first = true
}

# Task dependent configs.
coref {
  # Shared params.
  dropout_rate = ${main.dropout_rate}
  lexical_dropout_rate = ${main.lexical_dropout_rate}

  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  mention_ratio = 0.4

  # Model hyperparameters.
  loss_weight = 1.0 # Fixed. This is for weighted loss of each task in MTL.
  batch_size = 1 # Fixed. Current model can handle only the case batch_size = 1.
  filter_widths = [3, 4, 5]
  filter_size = 50
  ffnn_size = 150
  ffnn_depth = 2
  f_embedding_size = 20
  max_mention_width = 10
  use_metadata = true
  use_features = true
  model_heads = true

  # Dataset
  data_dir = dataset/coref/source
  conll_dir = dataset/coref/conll-2012
  train_data = train.english.jsonlines
  train_gold = train.english.v4_auto_conll
  valid_data = dev.english.jsonlines
  valid_gold = dev.english.v4_auto_conll
  test_data = test.english.jsonlines
  test_gold = test.english.v4_gold_conll
}

wikiP2D {
  batch_size = 10
  loss_weight = 1.0
  dropout_rate = ${main.dropout_rate}
  lexical_dropout_rate = ${main.lexical_dropout_rate}
  max_sent_length = {
    encode = 40
    decode = 20
  }
  n_triples = 0
  dataset = Q5O15000R300.half.bin
}

encoder{
}

adversarial{
  loss_weight = 1.0
}


# Global hyperparameters.
main {
  # Multi-task configuration.
  model_type = WeightedLoss
  coref_task = false
  graph_task = false
  desc_task = true
  adv_task = false

  # Learning hyperparameters.
  use_local_rnn = true
  rnn_size = 200

  #cell_type = GRUCell
  cell_type = CustomLSTMCell
  num_layers = 1
  max_to_keep = 10
  max_epoch = 60
  max_gradient_norm = 5.0
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  dropout_rate = 0.2          # keep_prob = 1.0 - rate
  lexical_dropout_rate = 0.5  # keep_prob = 1.0 - rate

  # Task specific configrations.
  coref = ${coref}
  wikiP2D = ${wikiP2D}
  adversarial = ${adversarial}

  # Word and character processing.
  cbase = true
  wbase = true
  w_vocab_size = 20000
  c_vocab_size = 1000
  w_embedding_size = 300
  c_embedding_size = 8
  
  lowercase = true
  trainable_emb = false

  #embeddings_conf = [${glove_300d_filtered}, ${turian_50d}]
  embeddings_conf = [${glove_300d_filtered}]
  char_vocab_path = ${embeddings_dir}/char_vocab.english.txt
}
